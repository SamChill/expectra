-catch_rsh /opt/gridengine/default/spool/compute-0-20/active_jobs/9698186.1/pe_hostfile
compute-0-20
compute-0-20
compute-0-20
compute-0-20
compute-0-20
compute-0-20
compute-0-20
compute-0-20
compute-0-20
compute-0-20
compute-0-20
compute-0-20
compute-0-20
compute-0-20
compute-0-20
compute-0-20
compute-0-20
compute-0-20
compute-0-20
compute-0-20
compute-0-20
compute-0-20
compute-0-20
compute-0-20
[compute-0-20.local:87085] mca_base_component_repository_open: unable to open mca_patcher_overwrite: /home/leili/local/lib/openmpi_2.0.1/lib/openmpi/mca_patcher_overwrite.so: undefined symbol: mca_patcher_base_patch_t_class (ignored)
[compute-0-20.local:87085] mca_base_component_repository_open: unable to open mca_shmem_mmap: /home/leili/local/lib/openmpi_2.0.1/lib/openmpi/mca_shmem_mmap.so: undefined symbol: opal_show_help (ignored)
[compute-0-20.local:87085] mca_base_component_repository_open: unable to open mca_shmem_posix: /home/leili/local/lib/openmpi_2.0.1/lib/openmpi/mca_shmem_posix.so: undefined symbol: opal_show_help (ignored)
[compute-0-20.local:87085] mca_base_component_repository_open: unable to open mca_shmem_sysv: /home/leili/local/lib/openmpi_2.0.1/lib/openmpi/mca_shmem_sysv.so: undefined symbol: opal_show_help (ignored)
--------------------------------------------------------------------------
It looks like opal_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during opal_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_shmem_base_select failed
  --> Returned value -1 instead of OPAL_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_init failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[compute-0-20.local:87085] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
